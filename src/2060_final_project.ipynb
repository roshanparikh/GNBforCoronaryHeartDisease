{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA 2060 Final Project**\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classification (GNB)\n",
    "\n",
    "Gaussian Naive Bayes (GNB) is a generative classification model. It assumes that the data for each feature is conditionally independent given the class label and that these features follow Gaussian (normal) distributions. Unlike standard Naive Bayes, which may handle discrete features, GNB specifically models continuous features using the normal distribution. The model makes predictions by combining probabilities from all features for each class. For a given input, the probabilities are calculated for each class, and the final classification is assigned to the class with the highest posterior probability.\n",
    "Formal equation:\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x}, y) = P_{\\theta}(y) \\prod_{i=1}^{d} P_{\\theta}(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Key Assumptions and Applications\n",
    "\n",
    "Due to our assumption that our features come from normal distributions, Gaussian Naive Bayes Classification is best suited for data with exclusively continuous variables, and where the features are not strongly correlated. This model also has some shortcomings, namely, it is vulnerable to datasets with outlier values that may greatly affect the mean and variance of the data, and large complex datasets where more complex models will typically perform better.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Parameter Estimation\n",
    "\n",
    "Gaussian Naive Bayes, being a **generative model**, does not use an optimizer function. Instead, it capitalizes on the naive assumption that our data comes from conditional independent normal distributions and uses **closed-form Maximum Likelihood Estimation (MLE)** to estimate parameters. MLE determines the parameters $ \\mu_y, \\sigma^2_y, P(y) $ that maximize the likelihood of the observed data. This is equivalent to minimizing the log loss. \n",
    "Formally: \n",
    "$$\n",
    "\\arg\\min_{\\theta} \\sum_{i=1}^{m} -\\log \\left[ P_{\\theta}(x_i, y_i) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Parameters Estimated:\n",
    "- **Class Priors:** $ P(y) $, the proportion of observations in each class.\n",
    "- **Feature Means:** $ \\mu_y $, the mean of each feature $ x_i $ given class $ y $.\n",
    "- **Feature Variances:** $ \\sigma^2_y $, the variance of each feature $ x_i $ given class $ y $.\n",
    "\n",
    "> **Note:** Unlike other Naive Bayes classifiers, GNB does not use Laplace smoothing because it works with continuous features. Instead, **variance smoothing** is applied by adding a very small constant (e.g., $ 10^{-6} $) to the variance to avoid instability when variance approaches zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction Process\n",
    "\n",
    "Now to make our predictions, we utilize our assumption that our data comes from normal distributions to calculate our predicted probabilities for each class. We calculate the predicted probabilities for each class $ y $ using the conditional probabilities for each feature $ x_i $, assuming normal distributions:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n",
    "$$\n",
    "\n",
    "### Steps:\n",
    "1. **Compute Conditional Probabilities:** For each feature $ x_i $ and class $ y $, calculate $ P(x_i \\mid y) $ using the Gaussian probability density function.\n",
    "2. **Get Joint Probabilities:** For each of the $n$ classes $ y $, compute the joint probability: $\\prod_{i=1}^{d} P(x_i \\mid y)$\n",
    "3. **Calculate Postiers:** Multiply our joint probabilities by the priors, then use logarithms for ease of computation:\n",
    "\n",
    "   $$\n",
    "   P(y \\mid x) \\propto P(y) \\prod_{i=1}^{d} P(x_i \\mid y)\\\\\n",
    "   \\Rightarrow \\log(P y\\mid x) \\propto \\log P(y) + \\log \\sum_{i=1}^n P(x \\mid y)\n",
    "   $$\n",
    "\n",
    "4. **Normalize Probabilities:** Convert the joint probabilities into valid probabilities by normalizing them to sum to 1.\n",
    "5. **Assign Class:** Select the class $ y $ with the highest posterior probability as the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance of Gaussian Naive Bayes can be evaluated using standard classification metrics, such as:\n",
    "- **Accuracy**\n",
    "- **Precision/Recall**\n",
    "- **Log Loss**\n",
    "\n",
    "For this explanation, **accuracy** is used as the primary evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayes(object):\n",
    "    \"\"\"Gaussian Naive Bayes model\n",
    "    \n",
    "    @attributes:\n",
    "        n_classes: number of classes\n",
    "            for our dataset, we will have 2 classes (yes heart disease and no heart disease)\n",
    "        attr_dist: n_classes x n_attributes NumPy array of attributes (2-D array)\n",
    "        label_priors: NumPy array of the priors distribution (1-D array)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        ''' \n",
    "        Notes here\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.attr_dist = None\n",
    "        self.label_priors = None\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        '''\n",
    "        Trains the model. Calculates label priors. Calculates mean, and variance for each class.\n",
    "        @params:\n",
    "            X_train: a 2D (n_examples x n_attributes) numpy array\n",
    "            y_train: a 1D (n_examples) numpy array\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        # Number of features\n",
    "        self.n_attributes = X_train.shape[1] \n",
    "\n",
    "        # Calculating the prior probability\n",
    "        self.label_priors = np.bincount(y_train, minlength=self.n_classes)/len(y_train)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def predict(self, inputs):\n",
    "        pass\n",
    "\n",
    "    def accuracy(self, X_test, y_test):\n",
    "        '''\n",
    "        Calculate 0-1 loss over predictions.\n",
    "        @params:\n",
    "            X_test: 2D array (n_examples x n_attributes) where each row is an example and each column is a feature/attribute\n",
    "            _test: 1D array where each entry corresponds to the label for a row in X\n",
    "        @return:\n",
    "            0-1 loss for the input data and associated labels\n",
    "        '''\n",
    "        acc = 0\n",
    "        predictions = self.predict(X_test)\n",
    "        for i in range(len(y_test)):\n",
    "            if predictions[i] == y_test[i]:\n",
    "                acc += 1\n",
    "        return acc/len(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Model\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
