{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Gaussian Naive Bayes with Coronary Heart Disease Data**\n",
    "-\n",
    "Roshan Parikh, Wali Siddiqui, Danielle Whisnant, Rio Wombacher\n",
    "-\n",
    "GitHub: https://github.com/roshanparikh/GNBforCoronaryHeartDisease.git\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classification (GNB)\n",
    "\n",
    "Gaussian Naive Bayes (GNB) is a generative classification model. It assumes that the data for each feature is conditionally independent given the class label and that these features follow Gaussian (normal) distributions. Unlike standard Naive Bayes, which may handle discrete features, GNB specifically models continuous features using the normal distribution. The model makes predictions by combining probabilities from all features for each class. For a given input, the probabilities are calculated for each class, and the final classification is assigned to the class with the highest posterior probability.\n",
    "Formal equation:\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x}, y) = P_{\\theta}(y) \\prod_{i=1}^{d} P_{\\theta}(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Key Assumptions and Applications\n",
    "\n",
    "Due to our assumption that our features come from normal distributions, Gaussian Naive Bayes Classification is best suited for data with exclusively continuous variables, and where the features are not strongly correlated. This model also has some shortcomings, namely, it is vulnerable to datasets with outlier values that may greatly affect the mean and variance of the data, and large complex datasets where more complex models will typically perform better.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Parameter Estimation\n",
    "\n",
    "Gaussian Naive Bayes, being a **generative model**, does not use an optimizer function. Instead, it capitalizes on the naive assumption that our data comes from conditional independent normal distributions and uses **closed-form Maximum Likelihood Estimation (MLE)** to estimate parameters. MLE determines the parameters $ \\mu_y, \\sigma^2_y, P(y) $ that maximize the likelihood of the observed data. This is equivalent to minimizing the log loss. \n",
    "Formally: \n",
    "$$\n",
    "\\arg\\min_{\\theta} \\sum_{i=1}^{m} -\\log \\left[ P_{\\theta}(x_i, y_i) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Parameters Estimated:\n",
    "- **Class Priors:** $ P(y) $, the proportion of observations in each class.\n",
    "- **Feature Means:** $ \\mu_y $, the mean of each feature $ x_i $ given class $ y $.\n",
    "- **Feature Variances:** $ \\sigma^2_y $, the variance of each feature $ x_i $ given class $ y $.\n",
    "\n",
    "> **Note:** Unlike other Naive Bayes classifiers, GNB does not use Laplace smoothing because it works with continuous features. Instead, **variance smoothing** is applied by adding a very small constant (e.g., $ 10^{-6} $) to the variance to avoid instability when variance approaches zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction Process\n",
    "\n",
    "Now to make our predictions, we utilize our assumption that our data comes from normal distributions to calculate our predicted probabilities for each class. We calculate the predicted probabilities for each class $ y $ using the conditional probabilities for each feature $ x_i $, assuming normal distributions:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\\\\\n",
    "\\Rightarrow\\log P(x_i, y) = -\\frac{1}{2}\\log (2\\pi\\sigma^2_y) - \\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\n",
    "$$\n",
    "\n",
    "### Steps:\n",
    "1. **Compute Conditional Probabilities:** For each feature $ x_i $ and class $ y $, calculate $ P(x_i \\mid y) $ using the Gaussian probability density function.\n",
    "2. **Get Joint Probabilities:** For each each class in $ y $, compute the joint probability: $\\prod_{i=1}^{d} P(x_i \\mid y)$\n",
    "3. **Calculate Postiers:** Multiply our joint probabilities by the priors, then use logarithms for ease of computation:\n",
    "\n",
    "   $$\n",
    "   P(y \\mid x) \\propto P(y) \\prod_{i=1}^{d} P(x_i \\mid y)\\\\\n",
    "   \\Rightarrow \\log(P y\\mid x) \\propto \\log P(y) + \\log \\sum_{i=1} P(x_i \\mid y)\n",
    "   $$\n",
    "\n",
    "4. **Normalize Probabilities:** Convert the joint probabilities into valid probabilities by normalizing them to sum to 1.\n",
    "5. **Assign Class:** Select the class $ y $ with the highest posterior probability as the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance of Gaussian Naive Bayes can be evaluated using standard classification metrics, such as:\n",
    "- **Accuracy**\n",
    "- **Precision/Recall**\n",
    "- **Log Loss**\n",
    "\n",
    "For this explanation, **accuracy** is used as the primary evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayes(object):\n",
    "    \"\"\"Gaussian Naive Bayes model\n",
    "    \n",
    "    @attributes:\n",
    "        n_classes: number of classes\n",
    "            for our dataset, we will have 2 classes (yes heart disease and no heart disease)\n",
    "        class_means: NumPy array of the means for each class\n",
    "        class_vars: NumPy array of the variances for each class\n",
    "        label_priors: NumPy array of the priors distribution (1-D array)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        ''' \n",
    "        Notes here\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.label_priors = None\n",
    "        self.class_means = None\n",
    "        self.class_vars = None\n",
    "\n",
    "    def train(self, X_train, y_train, alpha=1e-9):\n",
    "        '''\n",
    "        Trains the model. Calculates label priors. Calculates mean, and variance for each feature for each class.\n",
    "        @params:\n",
    "            X_train: a 2D (n_examples x n_attributes) numpy array\n",
    "            y_train: a 1D (n_examples) numpy array\n",
    "            alpha: adjustment for variance (scalar)\n",
    "        @return (to be used for unit tests):\n",
    "            label_priors: a 1-D numpy array with the prior distribution for each class\n",
    "            class_means: a 1-D numpy array with the means for each class\n",
    "            class_vars: a 1-D numpy array with the variances for each class\n",
    "        '''\n",
    "        # Input Validation\n",
    "        if not isinstance(X_train, np.ndarray):\n",
    "            raise TypeError(\"X_train must be a NumPy array.\")\n",
    "        if not isinstance(y_train, np.ndarray):\n",
    "            raise TypeError(\"y_train must be a NumPy array.\")\n",
    "        if X_train.shape[0] != y_train.shape[0]:\n",
    "            raise ValueError(\"Number of samples in X_train and y_train must be equal.\")\n",
    "        if X_train.size == 0:\n",
    "            raise ValueError(\"X_train is empty.\")\n",
    "        if y_train.size == 0:\n",
    "            raise ValueError(\"y_train is empty.\")\n",
    "\n",
    "        # Number of features\n",
    "        self.n_attributes = X_train.shape[1] \n",
    "\n",
    "        # Creating empty arrays of length n_classes to store class means and variances\n",
    "        class_means = np.zeros([self.n_classes, self.n_attributes])\n",
    "        class_vars = np.zeros([self.n_classes, self.n_attributes])\n",
    "\n",
    "        # Calculating the prior probability: P(y)\n",
    "        label_priors = np.bincount(y_train, minlength=self.n_classes)/len(y_train)\n",
    "\n",
    "        for n_class in range(self.n_classes):\n",
    "            # Getting X examples for specific classes\n",
    "            X_class = X_train[y_train == n_class]\n",
    "\n",
    "            if X_class.size == 0:\n",
    "                # Assign a small variance (alpha) to prevent computational issues if no data for a class\n",
    "                class_means[n_class] = np.zeros(self.n_attributes)\n",
    "                class_vars[n_class] = np.full(self.n_attributes, alpha)\n",
    "            else:\n",
    "                # Calculating mean (mu) and variance (sigma^2)\n",
    "                class_means[n_class] = (np.mean(X_class, axis = 0))\n",
    "                class_vars[n_class] = (np.var(X_class, axis = 0))\n",
    "\n",
    "        class_vars += alpha \n",
    "        \n",
    "        self.label_priors = label_priors\n",
    "        self.class_means = class_means\n",
    "        self.class_vars = class_vars\n",
    "\n",
    "        return label_priors, class_means, class_vars\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        ''' \n",
    "        @params:\n",
    "            X_test: 2-D array\n",
    "        @return:\n",
    "            predictions: 1-D array of length X_test \n",
    "        '''\n",
    "        predictions = np.zeros(X_test.shape[0])\n",
    "\n",
    "        # Log(P(y))\n",
    "        log_priors = np.log(self.label_priors)\n",
    "\n",
    "        for i,x in enumerate(X_test):\n",
    "            # P(y|x); np array of length n_classes\n",
    "            posteriors = np.zeros_like(log_priors) \n",
    "\n",
    "            for n_class in range(self.n_classes):\n",
    "                log_prior = log_priors[n_class]\n",
    "                mean = self.class_means[n_class]\n",
    "                var = self.class_vars[n_class]\n",
    "\n",
    "                log_likelihood = self.log_likelihood_func(x, mean, var)\n",
    "\n",
    "                # Calculates posterior distribution for this example x\n",
    "                posteriors[n_class] = (log_prior + log_likelihood)\n",
    "            \n",
    "            # Predicts the class by determining the index (class) with the highest posterior probability\n",
    "            predictions[i] = np.argmax(posteriors)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def log_likelihood_func(self, x, mean, var):\n",
    "        ''' \n",
    "        @params:\n",
    "            x: 1-D array (example)\n",
    "            mean: scalar\n",
    "            var: scalar\n",
    "        @returns:\n",
    "            log likelihood for example\n",
    "        '''\n",
    "\n",
    "        epsilon = 1e-10\n",
    "        var = np.maximum(var, epsilon)\n",
    "        log_likelihood = -0.5 * np.sum(\n",
    "        np.log(2 * np.pi * var) +  # log of variance term\n",
    "        ((x - mean) ** 2) / var    # squared difference normalized by variance\n",
    "        )\n",
    "    \n",
    "        return log_likelihood\n",
    "\n",
    "\n",
    "    def accuracy(self, X_test, y_test):\n",
    "        '''\n",
    "        Calculate 0-1 loss over predictions.\n",
    "        @params:\n",
    "            X_test: 2D array (n_examples x n_attributes) where each row is an example and each column is a feature/attribute\n",
    "            _test: 1D array where each entry corresponds to the label for a row in X\n",
    "        @return:\n",
    "            0-1 loss for the input data and associated labels\n",
    "        '''\n",
    "        predictions = self.predict(X_test)\n",
    "        return np.mean(predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Model\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing base cases\n",
    "import pytest\n",
    "#Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Models with 2 classes (yes heart disease and no heart disease)\n",
    "test_model1 = GaussianNaiveBayes(2)\n",
    "test_model2 = GaussianNaiveBayes(2)\n",
    "\n",
    "#Creates Data for test_model1 - base case \n",
    "x1 = np.array([\n",
    "    [1.0, 2.1, 3.2],\n",
    "    [1.1, 2.0, 3.1],\n",
    "    [1.2, 2.2, 3.0],\n",
    "    [4.0, 5.1, 6.2],\n",
    "    [4.1, 5.0, 6.1]\n",
    "])\n",
    "y1 = np.array([0, 0, 0, 1, 1])\n",
    "x_test1 = np.array([\n",
    "    [1.05, 2.05, 3.05],\n",
    "    [4.05, 5.05, 6.05],\n",
    "    [2.5, 3.5, 4.5]\n",
    "])\n",
    "y_test1 = np.array([0, 1, 0])\n",
    "\n",
    "#Creates data for test_model2 - base case \n",
    "x2 = np.array([\n",
    "    [2.0, 3.1, 4.2],\n",
    "    [2.1, 3.0, 4.1],\n",
    "    [2.2, 3.2, 4.0],\n",
    "    [5.0, 6.1, 7.2],\n",
    "    [5.1, 6.0, 7.1],\n",
    "    [5.2, 6.2, 7.0]\n",
    "])\n",
    "y2 = np.array([0, 0, 0, 1, 1, 1])\n",
    "x_test2 = np.array([\n",
    "    [2.05, 3.05, 4.05],\n",
    "    [5.05, 6.05, 7.05],\n",
    "    [3.5, 4.5, 5.5]\n",
    "])\n",
    "y_test2 = np.array([0, 1, 0])\n",
    "\n",
    "# Test Models\n",
    "def check_train_dtype(model, label_priors, class_means, class_vars, x_train):\n",
    "    assert isinstance(class_means, np.ndarray)\n",
    "    assert class_means.ndim == 2 and class_means.shape == (model.n_classes, x_train.shape[1])\n",
    "    assert isinstance(class_vars, np.ndarray)\n",
    "    assert class_vars.ndim == 2 and class_vars.shape == (model.n_classes, x_train.shape[1])\n",
    "    assert isinstance(label_priors, np.ndarray)\n",
    "    assert label_priors.ndim == 1 and label_priors.shape == (model.n_classes,)\n",
    "\n",
    "# Test Model 1 training\n",
    "label_priors1, class_means1, class_vars1 = test_model1.train(x1, y1)\n",
    "check_train_dtype(test_model1, label_priors1, class_means1, class_vars1, x1)\n",
    "\n",
    "# Expected values for Model 1 (calculated manually)\n",
    "expected_label_priors1 = np.array([0.6, 0.4])\n",
    "expected_class_means1 = np.array([\n",
    "    [1.1, 2.1, 3.1],  # Mean of class 0\n",
    "    [4.05, 5.05, 6.15]  # Mean of class 1\n",
    "])\n",
    "expected_class_vars1 = np.array([\n",
    "    [0.0066667, 0.0066667, 0.0066667],  # Variance of class 0\n",
    "    [0.0025, 0.0025, 0.0025]  # Variance of class 1\n",
    "])\n",
    "\n",
    "assert np.allclose(label_priors1, expected_label_priors1, atol=1e-2)\n",
    "assert np.allclose(class_means1, expected_class_means1, atol=1e-2)\n",
    "assert np.allclose(class_vars1, expected_class_vars1, atol=1e-2)\n",
    "\n",
    "# Test Model 2 training\n",
    "label_priors2, class_means2, class_vars2 = test_model2.train(x2, y2)\n",
    "check_train_dtype(test_model2, label_priors2, class_means2, class_vars2, x2)\n",
    "\n",
    "# Expected values for Model 2\n",
    "expected_label_priors2 = np.array([0.5, 0.5])\n",
    "expected_class_means2 = np.array([\n",
    "    [2.1, 3.1, 4.1],  # Mean of class 0\n",
    "    [5.1, 6.1, 7.1]  # Mean of class 1\n",
    "])\n",
    "expected_class_vars2 = np.array([\n",
    "    [0.0066667, 0.0066667, 0.0066667],  # Variance of class 0\n",
    "    [0.0066667, 0.0066667, 0.0066667]  # Variance of class 1\n",
    "])\n",
    "\n",
    "assert np.allclose(label_priors2, expected_label_priors2, atol=1e-2)\n",
    "assert np.allclose(class_means2, expected_class_means2, atol=1e-2)\n",
    "assert np.allclose(class_vars2, expected_class_vars2, atol=1e-2)\n",
    "\n",
    "# helper to confirm data types and shapes of predictions\n",
    "def check_test_dtype(predictions, x_test):\n",
    "    assert isinstance(predictions, np.ndarray)\n",
    "    assert predictions.ndim == 1 and predictions.shape == (x_test.shape[0],)\n",
    "\n",
    "# Test Model 1 predictions\n",
    "predictions1 = test_model1.predict(x_test1)\n",
    "check_test_dtype(predictions1, x_test1)\n",
    "expected_predictions1 = y_test1\n",
    "assert np.array_equal(predictions1, expected_predictions1)\n",
    "\n",
    "# Test Model 2 predictions\n",
    "predictions2 = test_model2.predict(x_test2)\n",
    "check_test_dtype(predictions2, x_test2)\n",
    "expected_predictions2 = y_test2\n",
    "assert np.array_equal(predictions2, expected_predictions2)\n",
    "\n",
    "# Test Model 1 accuracy\n",
    "accuracy1 = test_model1.accuracy(x_test1, y_test1)\n",
    "assert accuracy1 == pytest.approx(1.0, 0.01)\n",
    "\n",
    "# Test Model 2 accuracy\n",
    "accuracy2 = test_model2.accuracy(x_test2, y_test2)\n",
    "assert accuracy2 == pytest.approx(1.0, 0.01)\n",
    "\n",
    "#Test Model 1 log-likelihood\n",
    "for idx, x in enumerate(x_test1):\n",
    "    for n_class in range(test_model1.n_classes):\n",
    "        mean = class_means1[n_class]\n",
    "        var = class_vars1[n_class]\n",
    "            \n",
    "        # Compute expected log likelihood manually\n",
    "        adjusted_var = np.maximum(var, 1e-10)\n",
    "        expected_log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * adjusted_var) + ((x - mean) ** 2) / adjusted_var)\n",
    "            \n",
    "        # Compute log likelihood using the model's method\n",
    "        computed_log_likelihood = test_model1.log_likelihood_func(x, mean, var)\n",
    "        assert np.isclose(computed_log_likelihood, expected_log_likelihood, atol=1e-6)\n",
    "\n",
    "#Test Model 2 log-likelihood\n",
    "for idx, x in enumerate(x_test2):\n",
    "    for n_class in range(test_model2.n_classes):\n",
    "        mean = class_means2[n_class]\n",
    "        var = class_vars2[n_class]\n",
    "            \n",
    "        # Compute expected log likelihood manually\n",
    "        adjusted_var = np.maximum(var, 1e-10)\n",
    "        expected_log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * adjusted_var) + ((x - mean) ** 2) / adjusted_var)\n",
    "            \n",
    "        # Compute log likelihood using the model's method\n",
    "        computed_log_likelihood = test_model2.log_likelihood_func(x, mean, var)\n",
    "        assert np.isclose(computed_log_likelihood, expected_log_likelihood, atol=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "#Testing Edge Cases:\n",
    "#1. Tests Train on an empty feature set\n",
    "model_emp = GaussianNaiveBayes(2)\n",
    "X_train_emp = np.array([]).reshape(0, 3)  # 0 samples, 3 features\n",
    "y_train_emp = np.array([], dtype=int)\n",
    "with pytest.raises(ValueError):\n",
    "    model_emp.train(X_train_emp, y_train_emp)\n",
    "\n",
    "#2. Tests model with zero variance \n",
    "model_var = GaussianNaiveBayes(2)\n",
    "X_train_var = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [1.0, 2.0],\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [3.0, 4.0],\n",
    "    [3.0, 4.0]\n",
    "    ])\n",
    "y_train_var = np.array([0, 0, 0, 1, 1, 1])\n",
    "label_priors_var, class_means_var, class_vars = model_var.train(X_train_var, y_train_var)\n",
    " \n",
    "expected_class_vars = np.array([\n",
    "    [0.0, 0.0],  # Zero variance for class 0\n",
    "    [0.0, 0.0]   # Zero variance for class 1\n",
    "    ])\n",
    "    \n",
    "assert np.allclose(class_vars, expected_class_vars, atol=1e-9)\n",
    "\n",
    "X_test_var = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [2.0, 3.0]])\n",
    "\n",
    "predictions_var = model_var.predict(X_test_var)\n",
    "expected_predictions_var = np.array([0, 1, 0])\n",
    "assert np.array_equal(predictions_var, expected_predictions_var)\n",
    "\n",
    "#3. Tests model with more than two classes\n",
    "model3 = GaussianNaiveBayes(3)\n",
    "X_train3 = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [1.1, 2.1],\n",
    "        [1.2, 2.2],\n",
    "        [3.0, 4.0],\n",
    "        [3.1, 4.1],\n",
    "        [3.2, 4.2],\n",
    "        [5.0, 6.0],\n",
    "        [5.1, 6.1],\n",
    "        [5.2, 6.2]\n",
    "    ])\n",
    "y_train3 = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2], dtype=int)\n",
    "model3.train(X_train3, y_train3, alpha=1e-9)\n",
    "    \n",
    "X_test3 = np.array([\n",
    "        [1.05, 2.05],\n",
    "        [3.05, 4.05],\n",
    "        [5.05, 6.05]])\n",
    "y_test3 = np.array([0, 1, 2], dtype=int)\n",
    "    \n",
    "predictions3 = model3.predict(X_test3)\n",
    "expected_predictions3 = np.array([0, 1, 2])\n",
    "assert np.array_equal(predictions3, expected_predictions3)\n",
    "\n",
    "#4. Tests model when training data only has one single class\n",
    "modela = GaussianNaiveBayes(n_classes=2)\n",
    "X_traina = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [1.1, 2.1],\n",
    "        [1.2, 2.2]\n",
    "    ])\n",
    "y_traina = np.array([0, 0, 0], dtype=int)  # Only class 0\n",
    "    \n",
    "label_priorsa, class_meansa, class_varsa = modela.train(X_traina, y_traina, alpha=1e-9)\n",
    "check_train_dtype(modela, label_priorsa, class_meansa, class_varsa, X_traina)\n",
    "    \n",
    "expected_label_priorsa = np.array([1, 0])\n",
    "expected_class_meansa = np.array([\n",
    "        [1.1, 2.1],    # Mean of class 0\n",
    "        [1e-9, 1e-9]    # Mean of class 1 (adjusted by alpha)\n",
    "    ])\n",
    "expected_class_varsa = np.array([\n",
    "        [0.0066667, 0.0066667],  # Variance of class 0\n",
    "        [1e-9, 1e-9]              # Variance of class 1 (adjusted by alpha)\n",
    "    ])\n",
    "assert np.allclose(label_priorsa, expected_label_priorsa, atol=1e-2)\n",
    "assert np.allclose(class_meansa, expected_class_meansa, atol=1e-2)\n",
    "assert np.allclose(class_varsa, expected_class_varsa, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Gaussian Naive Bayes Accuracy: 85.25%\n"
     ]
    }
   ],
   "source": [
    "#Testing the dataset using SKlearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Our Model splitting the data the same way done in SKlearn\n",
    "df = pd.read_csv('processed_cleveland.csv')\n",
    "\n",
    "df['num'] = (df['num'] > 0).astype(int)\n",
    "\n",
    "# Categorical features to encode\n",
    "categorical_features = ['cp', 'restecg', 'slope', 'thal', 'ca']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    df[feature] = pd.Categorical(df[feature]).codes\n",
    "\n",
    "# Remove any rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['num'])  \n",
    "y = df['num']  \n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=48)\n",
    "\n",
    "# Initialize and train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Sklearn Gaussian Naive Bayes Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model splitting like the dataset as done in sklearn: 85.25%\n"
     ]
    }
   ],
   "source": [
    "#Testing our model using train_test_split (as done in SKlearn above)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=48)\n",
    "\n",
    "gnb = GaussianNaiveBayes(n_classes=2)\n",
    "\n",
    "gnb.train(X_train, y_train, alpha=1e-9)\n",
    "\n",
    "accuracy = gnb.accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy of our model splitting like the dataset as done in sklearn: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Gaussian Naive Bayes Accuracy using 80/20 rule: 73.77%\n"
     ]
    }
   ],
   "source": [
    "# Manual 80/20 split in SKlearn\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train = X.values[:train_size]\n",
    "y_train = y.values[:train_size]\n",
    "X_test = X.values[train_size:]\n",
    "y_test = y.values[train_size:]\n",
    "\n",
    "sklearn_gnb = GaussianNB()\n",
    "sklearn_gnb.fit(X_train, y_train)\n",
    "sklearn_accuracy = sklearn_gnb.score(X_test, y_test)\n",
    "print(f\"Scikit-learn Gaussian Naive Bayes Accuracy using 80/20 rule: {sklearn_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model splitting like the dataset using the 80/20 rule: 73.77%\n"
     ]
    }
   ],
   "source": [
    "#80/20 Split Using Our Methods\n",
    "gnb = GaussianNaiveBayes(n_classes=2)\n",
    "\n",
    "alpha = 1e-9\n",
    "gnb.train(X_train, y_train, alpha)\n",
    "\n",
    "accuracy = gnb.accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy of our model splitting like the dataset using the 80/20 rule: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Models\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of our Gaussian Naive Bayes model, we compared it to the GaussianNB implementation from scikit-learn, using a dataset containing features relevant to diagnosing cardiovascular issues. The dataset used in this evaluation is a heart disease dataset, which contains various clinical and diagnostic features used to predict the presence or absence of coronary heart disease in patients. It consists of both continuous and categorical attributes that are essential for medical decision-making. The information on this dataset and testing came from \"Machine-Learning-Based Prediction Models of Coronary Heart Disease Using Naïve Bayes and Random Forest Algorithms,\" which used the Cleveland Database from the UCI Repository (Bemando, et al.).\n",
    "\n",
    "Both models rely on the assumption that continuous features are distributed according to a Gaussian (normal) distribution, which aligns the theoretical underpinnings of our implementation with those of scikit-learn.\n",
    "\n",
    "We began by preprocessing the dataset to ensure it was in a format suitable for analysis. Non-numeric categorical features were converted into numeric representations where necessary, and any missing values were addressed. The dataset was then split into training and test subsets to evaluate model performance consistently.\n",
    "\n",
    "For scikit-learn’s model, we utilized the GaussianNB class, which provides a ready-made implementation of the Gaussian Naive Bayes algorithm. The model was trained on the same training data as our custom implementation. After training, both models were tested on the same test set to predict the target labels.\n",
    "\n",
    "The accuracy of each model was calculated by comparing predicted labels to the actual test set labels. This allowed us to directly compare the performance of our implementation with scikit-learn's. We compared our model with sklearn's model in two ways. The first was by splitting the dataset for both our implementation and scikit-learn's using their built-in train_test_split method. This function randomly shuffles the dataset before splitting, ensuring that the training and testing sets are representative of the overall data distribution. The random_state parameter ensures the shuffle is reproducible. We selected random_state=48 for this to ensure our results were consistent. When doing this for both our model and scikit-learn's with random splitting, we get an accuracy of 85.25% for both models. This is also consistent with the paper referenced for testing, which is getting an accuracy of 85% for their Gaussian Naive Bayes model. When we do this through a more brute-force method without shuffling, we get an accuracy of 73.77% for both of them.\n",
    "\n",
    "This comparison ensured that our model was correctly capturing the Gaussian Naive Bayes principles and performing on par with the well-established library implementation of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baladram, S. (2024) Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners, Medium. Available at: https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c (Accessed: 10 December 2024).\n",
    "\n",
    "Bemando, Miranda, and Aryuni (2021) ‘Machine-Learning-Based Prediction Models of Coronary Heart disease Using Naïve Bayes and Random Forest Algorithms,’ IEEE Available at: https://ieeexplore.ieee.org/document/9537060 (Accessed 12 December 2024).\n",
    "\n",
    "Gaussian Naive Bayes (2023) GeeksforGeeks. Available at: https://www.geeksforgeeks.org/gaussian-naive-bayes/(Accessed: 10 December 2024).\n",
    "\n",
    "GaussianNB (no date) scikit-learn. Available at: https://scikit-learn/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html (Accessed: 10 December 2024).\n",
    "\n",
    "Kashishdafe (2024) ‘Gaussian Naive Bayes: Understanding the Basics and Applications’, Medium, 23 March. Available at: https://medium.com/@kashishdafe0410/gaussian-naive-bayes-understanding-the-basics-and-applications-52098087b963 (Accessed: 10 December 2024).\n",
    "\n",
    "‘Naive Bayes classifier’ (2024) Wikipedia. Available at: https://en.wikipedia.org/w/index.php?title=Naive_Bayes_classifier&oldid=1260034546 (Accessed: 10 December 2024).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
