{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA 2060 Final Project**\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classification (GNB)\n",
    "\n",
    "Gaussian Naive Bayes (GNB) is a generative classification model. It assumes that the data for each feature is conditionally independent given the class label and that these features follow Gaussian (normal) distributions. Unlike standard Naive Bayes, which may handle discrete features, GNB specifically models continuous features using the normal distribution. The model makes predictions by combining probabilities from all features for each class. For a given input, the probabilities are calculated for each class, and the final classification is assigned to the class with the highest posterior probability.\n",
    "Formal equation:\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x}, y) = P_{\\theta}(y) \\prod_{i=1}^{d} P_{\\theta}(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Key Assumptions and Applications\n",
    "\n",
    "Due to our assumption that our features come from normal distributions, Gaussian Naive Bayes Classification is best suited for data with exclusively continuous variables, and where the features are not strongly correlated. This model also has some shortcomings, namely, it is vulnerable to datasets with outlier values that may greatly affect the mean and variance of the data, and large complex datasets where more complex models will typically perform better.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Parameter Estimation\n",
    "\n",
    "Gaussian Naive Bayes, being a **generative model**, does not use an optimizer function. Instead, it capitalizes on the naive assumption that our data comes from conditional independent normal distributions and uses **closed-form Maximum Likelihood Estimation (MLE)** to estimate parameters. MLE determines the parameters $ \\mu_y, \\sigma^2_y, P(y) $ that maximize the likelihood of the observed data. This is equivalent to minimizing the log loss. \n",
    "Formally: \n",
    "$$\n",
    "\\arg\\min_{\\theta} \\sum_{i=1}^{m} -\\log \\left[ P_{\\theta}(x_i, y_i) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Parameters Estimated:\n",
    "- **Class Priors:** $ P(y) $, the proportion of observations in each class.\n",
    "- **Feature Means:** $ \\mu_y $, the mean of each feature $ x_i $ given class $ y $.\n",
    "- **Feature Variances:** $ \\sigma^2_y $, the variance of each feature $ x_i $ given class $ y $.\n",
    "\n",
    "> **Note:** Unlike other Naive Bayes classifiers, GNB does not use Laplace smoothing because it works with continuous features. Instead, **variance smoothing** is applied by adding a very small constant (e.g., $ 10^{-6} $) to the variance to avoid instability when variance approaches zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction Process\n",
    "\n",
    "Now to make our predictions, we utilize our assumption that our data comes from normal distributions to calculate our predicted probabilities for each class. We calculate the predicted probabilities for each class $ y $ using the conditional probabilities for each feature $ x_i $, assuming normal distributions:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\\\\\n",
    "\\Rightarrow\\log P(x_i, y) = -\\frac{1}{2}\\log (2\\pi\\sigma^2_y) - \\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\n",
    "$$\n",
    "\n",
    "### Steps:\n",
    "1. **Compute Conditional Probabilities:** For each feature $ x_i $ and class $ y $, calculate $ P(x_i \\mid y) $ using the Gaussian probability density function.\n",
    "2. **Get Joint Probabilities:** For each each class in $ y $, compute the joint probability: $\\prod_{i=1}^{d} P(x_i \\mid y)$\n",
    "3. **Calculate Postiers:** Multiply our joint probabilities by the priors, then use logarithms for ease of computation:\n",
    "\n",
    "   $$\n",
    "   P(y \\mid x) \\propto P(y) \\prod_{i=1}^{d} P(x_i \\mid y)\\\\\n",
    "   \\Rightarrow \\log(P y\\mid x) \\propto \\log P(y) + \\log \\sum_{i=1} P(x_i \\mid y)\n",
    "   $$\n",
    "\n",
    "4. **Normalize Probabilities:** Convert the joint probabilities into valid probabilities by normalizing them to sum to 1.\n",
    "5. **Assign Class:** Select the class $ y $ with the highest posterior probability as the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance of Gaussian Naive Bayes can be evaluated using standard classification metrics, such as:\n",
    "- **Accuracy**\n",
    "- **Precision/Recall**\n",
    "- **Log Loss**\n",
    "\n",
    "For this explanation, **accuracy** is used as the primary evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayes(object):\n",
    "    \"\"\"Gaussian Naive Bayes model\n",
    "    \n",
    "    @attributes:\n",
    "        n_classes: number of classes\n",
    "            for our dataset, we will have 2 classes (yes heart disease and no heart disease)\n",
    "        class_means: NumPy array of the means for each class\n",
    "        class_vars: NumPy array of the variances for each class\n",
    "        label_priors: NumPy array of the priors distribution (1-D array)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        ''' \n",
    "        Notes here\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.label_priors = None\n",
    "        self.class_means = None\n",
    "        self.class_vars = None\n",
    "\n",
    "    def train(self, X_train, y_train, alpha):\n",
    "        '''\n",
    "        Trains the model. Calculates label priors. Calculates mean, and variance for each feature for each class.\n",
    "        @params:\n",
    "            X_train: a 2D (n_examples x n_attributes) numpy array\n",
    "            y_train: a 1D (n_examples) numpy array\n",
    "            alpha: adjustment for variance (scalar)\n",
    "        @return (to be used for unit tests):\n",
    "            label_priors: a 1-D numpy array with the prior distribution for each class\n",
    "            class_means: a 1-D numpy array with the means for each class\n",
    "            class_vars: a 1-D numpy array with the variances for each class\n",
    "        '''\n",
    "        # Number of features\n",
    "        self.n_attributes = X_train.shape[1] \n",
    "\n",
    "        # Creating empty arrays of length n_classes to store class means and variances\n",
    "        class_means = np.zeros([self.n_classes, self.n_attributes])\n",
    "        class_vars = np.zeros([self.n_classes, self.n_attributes])\n",
    "\n",
    "        # Calculating the prior probability: P(y)\n",
    "        label_priors = np.bincount(y_train, minlength=self.n_classes)/len(y_train)\n",
    "\n",
    "        for n_class in range(self.n_classes):\n",
    "            # Getting X examples for specific classes\n",
    "            X_class = X_train[y_train == n_class]\n",
    "\n",
    "            # Calculating mean (mu) and variance (sigma^2)\n",
    "            class_means[n_class] = (np.mean(X_class, axis = 0))\n",
    "            class_vars[n_class] = (np.var(X_class, axis = 0))\n",
    "        \n",
    "        class_means += alpha\n",
    "\n",
    "        self.label_priors = label_priors\n",
    "        self.class_means = class_means\n",
    "        self.class_vars = class_vars\n",
    "\n",
    "        return label_priors, class_means, class_vars\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        ''' \n",
    "        @params:\n",
    "            X_test: 2-D array\n",
    "        @return:\n",
    "            predictions: 1-D array of length X_test \n",
    "        '''\n",
    "        predictions = np.zeros(X_test.shape[0])\n",
    "\n",
    "        # Log(P(y))\n",
    "        log_priors = np.log(self.label_priors)\n",
    "\n",
    "        for i,x in enumerate(X_test):\n",
    "            # P(y|x); np array of length n_classes\n",
    "            posteriors = np.zeros_like(log_priors) \n",
    "\n",
    "            for n_class in range(self.n_classes):\n",
    "                log_prior = log_priors[n_class]\n",
    "                mean = self.class_means[n_class]\n",
    "                var = self.class_vars[n_class]\n",
    "\n",
    "                log_likelihood = self.log_likelihood_func(x, mean, var)\n",
    "\n",
    "                # Calculates posterior distribution for this example x\n",
    "                posteriors[n_class] = (log_prior + log_likelihood)\n",
    "            \n",
    "            # Predicts the class by determining the index (class) with the highest posterior probability\n",
    "            predictions[i] = np.argmax(posteriors)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def log_likelihood_func(self, x, mean, var):\n",
    "        ''' \n",
    "        @params:\n",
    "            x: 1-D array (example)\n",
    "            mean: scalar\n",
    "            var: scalar\n",
    "        @returns:\n",
    "            log likelihood for example\n",
    "        '''\n",
    "\n",
    "        epsilon = 1e-10\n",
    "        var = np.maximum(var, epsilon)\n",
    "        log_likelihood = -0.5 * np.sum(\n",
    "        np.log(2 * np.pi * var) +  # log of variance term\n",
    "        ((x - mean) ** 2) / var    # squared difference normalized by variance\n",
    "        )\n",
    "    \n",
    "        return log_likelihood\n",
    "\n",
    "\n",
    "    def accuracy(self, X_test, y_test):\n",
    "        '''\n",
    "        Calculate 0-1 loss over predictions.\n",
    "        @params:\n",
    "            X_test: 2D array (n_examples x n_attributes) where each row is an example and each column is a feature/attribute\n",
    "            _test: 1D array where each entry corresponds to the label for a row in X\n",
    "        @return:\n",
    "            0-1 loss for the input data and associated labels\n",
    "        '''\n",
    "        predictions = self.predict(X_test)\n",
    "        return np.mean(predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Model\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "#Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Models with 2 classes (yes heart disease and no heart disease)\n",
    "test_model1 = GaussianNaiveBayes(2)\n",
    "test_model2 = GaussianNaiveBayes(2)\n",
    "\n",
    "#Creates Data for test_model1\n",
    "x1 = np.array([\n",
    "    [1.0, 2.1, 3.2],\n",
    "    [1.1, 2.0, 3.1],\n",
    "    [1.2, 2.2, 3.0],\n",
    "    [4.0, 5.1, 6.2],\n",
    "    [4.1, 5.0, 6.1]\n",
    "])\n",
    "y1 = np.array([0, 0, 0, 1, 1])\n",
    "x_test1 = np.array([\n",
    "    [1.05, 2.05, 3.05],\n",
    "    [4.05, 5.05, 6.05],\n",
    "    [2.5, 3.5, 4.5]\n",
    "])\n",
    "y_test1 = np.array([0, 1, 0])\n",
    "\n",
    "#Creates data for test_model2\n",
    "x2 = np.array([\n",
    "    [2.0, 3.1, 4.2],\n",
    "    [2.1, 3.0, 4.1],\n",
    "    [2.2, 3.2, 4.0],\n",
    "    [5.0, 6.1, 7.2],\n",
    "    [5.1, 6.0, 7.1],\n",
    "    [5.2, 6.2, 7.0]\n",
    "])\n",
    "y2 = np.array([0, 0, 0, 1, 1, 1])\n",
    "x_test2 = np.array([\n",
    "    [2.05, 3.05, 4.05],\n",
    "    [5.05, 6.05, 7.05],\n",
    "    [3.5, 4.5, 5.5]\n",
    "])\n",
    "y_test2 = np.array([0, 1, 0])\n",
    "\n",
    "# Test Models\n",
    "def check_train_dtype(model, label_priors, class_means, class_vars, x_train):\n",
    "    assert isinstance(class_means, np.ndarray)\n",
    "    assert class_means.ndim == 2 and class_means.shape == (model.n_classes, x_train.shape[1])\n",
    "    assert isinstance(class_vars, np.ndarray)\n",
    "    assert class_vars.ndim == 2 and class_vars.shape == (model.n_classes, x_train.shape[1])\n",
    "    assert isinstance(label_priors, np.ndarray)\n",
    "    assert label_priors.ndim == 1 and label_priors.shape == (model.n_classes,)\n",
    "\n",
    "# Test Model 1 training\n",
    "label_priors1, class_means1, class_vars1 = test_model1.train(x1, y1, alpha=1e-9)\n",
    "check_train_dtype(test_model1, label_priors1, class_means1, class_vars1, x1)\n",
    "\n",
    "# Expected values for Model 1 (calculated manually)\n",
    "expected_label_priors1 = np.array([0.6, 0.4])\n",
    "expected_class_means1 = np.array([\n",
    "    [1.1, 2.1, 3.1],  # Mean of class 0\n",
    "    [4.05, 5.05, 6.15]  # Mean of class 1\n",
    "])\n",
    "expected_class_vars1 = np.array([\n",
    "    [0.0066667, 0.0066667, 0.0066667],  # Variance of class 0\n",
    "    [0.0025, 0.0025, 0.0025]  # Variance of class 1\n",
    "])\n",
    "\n",
    "\n",
    "assert np.allclose(label_priors1, expected_label_priors1, atol=1e-2)\n",
    "assert np.allclose(class_means1, expected_class_means1, atol=1e-2)\n",
    "assert np.allclose(class_vars1, expected_class_vars1, atol=1e-2)\n",
    "\n",
    "# Test Model 2 training\n",
    "label_priors2, class_means2, class_vars2 = test_model2.train(x2, y2, alpha=1e-9)\n",
    "check_train_dtype(test_model2, label_priors2, class_means2, class_vars2, x2)\n",
    "\n",
    "# Expected values for Model 2\n",
    "expected_label_priors2 = np.array([0.5, 0.5])\n",
    "expected_class_means2 = np.array([\n",
    "    [2.1, 3.1, 4.1],  # Mean of class 0\n",
    "    [5.1, 6.1, 7.1]  # Mean of class 1\n",
    "])\n",
    "expected_class_vars2 = np.array([\n",
    "    [0.0066667, 0.0066667, 0.0066667],  # Variance of class 0\n",
    "    [0.0066667, 0.0066667, 0.0066667]  # Variance of class 1\n",
    "])\n",
    "\n",
    "assert np.allclose(label_priors2, expected_label_priors2, atol=1e-2)\n",
    "assert np.allclose(class_means2, expected_class_means2, atol=1e-2)\n",
    "assert np.allclose(class_vars2, expected_class_vars2, atol=1e-2)\n",
    "\n",
    "# Define a function to check the data types and shapes of predictions\n",
    "def check_test_dtype(predictions, x_test):\n",
    "    assert isinstance(predictions, np.ndarray)\n",
    "    assert predictions.ndim == 1 and predictions.shape == (x_test.shape[0],)\n",
    "\n",
    "# Test Model 1 predictions\n",
    "predictions1 = test_model1.predict(x_test1)\n",
    "check_test_dtype(predictions1, x_test1)\n",
    "expected_predictions1 = y_test1\n",
    "assert np.array_equal(predictions1, expected_predictions1)\n",
    "\n",
    "# Test Model 2 predictions\n",
    "predictions2 = test_model2.predict(x_test2)\n",
    "check_test_dtype(predictions2, x_test2)\n",
    "expected_predictions2 = y_test2\n",
    "assert np.array_equal(predictions2, expected_predictions2)\n",
    "\n",
    "# Test Model 1 accuracy\n",
    "accuracy1 = test_model1.accuracy(x_test1, y_test1)\n",
    "assert accuracy1 == pytest.approx(1.0, 0.01)\n",
    "\n",
    "# Test Model 2 accuracy\n",
    "accuracy2 = test_model2.accuracy(x_test2, y_test2)\n",
    "assert accuracy2 == pytest.approx(1.0, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.61%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('processed_cleveland.csv')\n",
    "\n",
    "df['num'] = (df['num'] > 0).astype(int)\n",
    "\n",
    "# Categorical features to encode\n",
    "categorical_features = ['cp', 'restecg', 'slope', 'thal', 'ca']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    df[feature] = pd.Categorical(df[feature]).codes\n",
    "\n",
    "# Remove any rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['num'])  \n",
    "y = df['num']  \n",
    "\n",
    "# Split the data\n",
    "# train_size = int(0.8 * len(X))\n",
    "# X_train = X.values[:train_size]\n",
    "# y_train = y.values[:train_size]\n",
    "# X_test = X.values[train_size:]\n",
    "# y_test = y.values[train_size:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "\n",
    "gnb = GaussianNaiveBayes(n_classes=2)\n",
    "\n",
    "alpha = 1e-9\n",
    "gnb.train(X_train, y_train, alpha)\n",
    "\n",
    "accuracy = gnb.accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Gaussian Naive Bayes Accuracy: 83.61%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Sklearn Gaussian Naive Bayes Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Gaussian Naive Bayes Accuracy: 73.77%\n"
     ]
    }
   ],
   "source": [
    "# Manual 80/20 split\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train = X.values[:train_size]\n",
    "y_train = y.values[:train_size]\n",
    "X_test = X.values[train_size:]\n",
    "y_test = y.values[train_size:]\n",
    "\n",
    "\n",
    "sklearn_gnb = GaussianNB()\n",
    "sklearn_gnb.fit(X_train, y_train)\n",
    "sklearn_accuracy = sklearn_gnb.score(X_test, y_test)\n",
    "print(f\"Scikit-learn Gaussian Naive Bayes Accuracy: {sklearn_accuracy * 100:.2f}%\")\n",
    "\n",
    "#This is what we were getting before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
